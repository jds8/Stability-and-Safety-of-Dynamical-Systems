:PROPERTIES:
:ID:       610b22b2-e8e4-40a0-82d3-6d3514d1fa3f
:END:
#+TITLE: Stability and Safety of Dynamical Systems
#+AUTHOR: Justice Sefas
#+OPTIONS: toc:nil num:nil tex:t html-postamble:nil
#+OPTIONS: broken-links:t

#+LATEX_HEADER: \usepackage{amsfonts}
#+LATEX_HEADER: \usepackage{physics}

#+LATEX_HEADER: \usepackage{bbm}

#+LATEX_HEADER: \usepackage{amsthm}

#+LATEX_HEADER: \setlength{\parindent}{0pt}

* Stability and Safety of Dynamical Systems
** Uncontrolled Dynamical System
*** Stochastic
**** Continuous Time
***** Continuous Space
****** Stochastic Differential Equation
***** Discrete Space
****** Continuous-time Markov Chain
**** Discrete Time
***** Continuous Space
****** Markov Chain
- e.g. [[id:54ff2c21-d097-4323-b3ab-1638d3d55680][SMC]]/Particle Filter, [[id:296494de-31f1-49e9-b26f-67a8a856e618][linear gaussian model]]
******* Linear Gaussian Systems: $z_{t+1} = Az_t + w_t, w_t \sim N(\mu_z, \Sigma_z)$ and $x_t = Cz_t + v_t, v_t \sim N(\mu_x, \Sigma_x)$
- In such models, the [[id:37fdcbef-2dae-41c1-85b5-b2c429a92332][Filtering Distribution]] and [[id:165302de-a963-49ae-8128-29052722facf][Smoothing Posterior]] can be found exactly since the [[id:8ec828b1-3e78-4058-afa3-b0d3314c2059][Marginal Likelihood]] can be derived analytically
******* Nonlinear Gaussian Systems: $x_k = f(x_{k-1}, u_k) + w_k$ and $z_k = h(x_k) + v_k$ for nonlinear $f,h$
- [[id:f56999b3-c582-4fe4-8a2e-4765f888dc07][Sequential Monte Carlo]]
******* Under what conditions can we show that the system is stable?
- Foster-Lyapunov Functions
***** Discrete Space
****** Markov Chain
- e.g. Hidden Markov Model
  - Viterbi algorithm finds the [[id:37f3fbc1-6242-4e75-bed8-acd669e33418][MAP]] of the most likely sequence for a given set of observations
  - The Forward-Backward algorithm computes the smoothing distribution
******* Under what conditions can we show that the system is stable?
- Foster-Lyapunov Functions
*** Deterministic
**** Continuous Time
***** Continuous Space
****** Ordinary Differential Equations
******* Under what conditions can we show that the system is stable?
******** Linear Systems: dx/dt = Ax
- If the real part of all eigenvalues of A are negative, the system is stable.
******** Nonlinear Systems: dx/dt = f(x)
- We cannot know if the system is *globally* stable, but we can check if *equilibrium points* (x values where dx/dt = 0) are *locally* stable by computing the first order Taylor approximation (“linearizing”) and checking that all of the eigenvalues of the resultant Jacobian are negative. If not, we either have a *saddle point* or *local maximum*.
  - If one of the eigenvalues of the Jacobian is positive, then the system diverges from the equilibrium point at which time the linearization no longer holds.
- Alternatively, the existence of a [[id:29ae2cd5-d575-49d3-b1fe-2c9afd071a61][Lyapunov Function]] is a *necessary and sufficient condition* for stability
********* In what cases can we find a Lyapunov function?
- IDK, look for square terms
******** References
- https://physiology.med.cornell.edu/people/banfelder/qbio/resources_2008/4.2_Differential_Equations.pdf
****** Partial Differential Equations
******* Under what conditions can we show that the system is stable?
******** Linear Systems: L(u) = f where L is a linear operator (i.e. it only involves linear functions of u and its derivatives) and f doesn't depend on u
- [[https://math.mit.edu/classes/18.311/WWW2011/Notes/NumSchemeStab.pdf][von Neumann stability analysis (Fourier stability analysis)]]
******** Nonlinear Systems:
- [[https://en.wikipedia.org/wiki/Lax_equivalence_theorem][Lax equivalence theorem (convergent <==> stable)]]
***** Discrete Space
****** Not Sure
**** Discrete Time
***** Continuous Space
****** Difference Equations
******* Under what conditions can we show that the system is stable?
******** Linear Systems: x(t+1) = Ax(t)
- If the *absolute value of the eigenvalues* of $A$ are all *less than 1* (note not less than 0 as in the ODE case), then the system is stable
******** Nonlinear Systems: dx/dt = f(x)
***** Discrete Space
****** Finite Automata

** Controlled Dynamical System
*** Stochastic
**** Continuous Time
***** Continuous Space
****** Controlled Stochastic Differential Equation
****** Continuous-time Markov Decision Process
***** Discrete Space
****** Continuous-time Markov Decision Process
****** Discrete-event Dynamic system
**** Discrete Time
***** Continuous Space
****** Markov Decision Process
******* Continuous Action/Control
******** [[id:3a03b503-13d4-4d4d-907b-529c2337ad8d][Reinforcement Learning]]
- [[id:ceb63e44-3a2b-4367-ab79-9dd17855e40e][Policy Gradient Methods]]
  - e.g. [[id:d8a9234b-398f-4ad5-ad3d-130e4b63c307][TRPO,]] [[id:99d0f2e2-f8ff-43a2-8a53-7f603eb16a2b][PPO]], [[id:5f12417c-ba57-484c-819d-d9f7f68e7694][SAC]], DDPG
  - https://lilianweng.github.io/posts/2018-04-08-policy-gradient/
- we can construct a *conservatively discretized* "safety MDP" from the original continuous-state MDP and synthesize a controller using linear temporal logic which satisfies guarantees
  - https://hanspeterschaub.info/Papers/Harris2020.pdf works in POMDPs as well
  - when does this fail?
    - in what cases can we *not construct* a conservatively discretized safety MDP?
******** What can we do in continuous action spaces that we cannot do in discrete action spaces?
******** Under what conditions can we show the system is safe?
- if the *state space is compact (closed and bounded)* we can discretize it and use a [[id:50579a3f-8ff0-49ec-9614-f629a508b657][Reach-Avoid Supermartingale]] if the dynamics is [[id:2cce077f-52f2-4dd4-b1e0-9728094988ea][lipschitz continuous]] to show the system is *safe up to some probability*
  - https://arxiv.org/abs/2210.05308
    - under what conditions does this fail?
      - "in general, it is not possible to compute a closed form expression for the expected value of an RASM over successor system states, as both the policy and the RASM are neural networks."
  - https://arxiv.org/abs/2312.01456
******* Discrete Action/Control
******** What can we do in discrete action spaces that we cannot do in continuous action spaces?
- Deep Q-learning ([[id:ce319b92-d08a-4d74-ad96-49a402c27df6][DQN]]) uses [[id:70b85ac1-3c1a-45c1-80fe-55f83e36eb6c][Temporal Differencing]] as its loss and outputs a softmax over all possible actions
  - note that DQN cannot keep a state-action table as in Q-Learning because the state space is continuous
- construct a "probabilistic [[id:443299d0-541d-4889-acc1-98ad9ce8da5c][shield]]" by enumerating over all possible actions
  - note that this *requires a discrete state abstraction* (see Shielding section under Discrete Space below)
  - https://arxiv.org/pdf/2303.03226.pdf
****** POMDP
***** Discrete Space
****** Markov Decision Process
******* Continuous Action/Control
******** What can we do in continuous action spaces that we cannot do in discrete action spaces?
******* Discrete Action/Control
******** What can we do in discrete action spaces that we cannot do in continuous action spaces?
********* Shielding
- shielding creates a finite state and action MDP abstraction from a (potentially) continuous state and action MDP
- linear temporal logic specifications are placed on finite MDP and reactive synthesis tools such as [[id:e288c7ff-72b4-4ae6-9d32-d298cc9f1f7e][PRISM]] are used for model checking to synthesize a policy (through dynamic programming?)
  - https://hanspeterschaub.info/Papers/Harris2020.pdf is done in a POMDP
********* [[id:2882a7e4-9410-47eb-85da-70d02db6b502][On-Policy]] Algorithms
- SARSA is very similar to Q-learning (see below), but it updates its Q-table according to the [[id:91bca60c-f545-465c-93f5-b05cf15e6c18][epsilon-greedy algorithm]] it uses to take actions
  - SARSA *does not use or learn* a model (i.e., it's [[id:2282c6fe-3571-4a62-9daf-4065e649a70c][Model Free RL]]) of the dynamics yet converges to an epsilon-greedy policy, which is "very close" to optimal
********* [[id:e2b4c1b2-9505-463b-ab93-723c9353e702][Off-Policy]] Algorithms
- [[id:d424f0b7-3d47-4f69-9198-856bd2c4be75][Q-Learning]] maintains a state-action table which is *only possible in the discrete space, discrete action regime*
  - Moreover, Q-learning:
    - *does not use or learn* a model of the dynamics (i.e, it's [[id:2282c6fe-3571-4a62-9daf-4065e649a70c][Model Free RL]]) yet converges to the optimal policy *if the optimal policy is deterministic*
    - Q-learning is off-policy because it updates its Q-table deterministically using the argmax action yet samples actions according to a *different policy* (an epsilon-greedy policy) *during training*
****** POMDP
*** Deterministic
**** Continuous Time
***** Continuous Space
****** Controlled Dynamical System
******* [[id:9f983716-9845-4d7b-9d9e-927182ad0a88][Classical Control]]
******** Kalman-Bucy Filter
- continuous-time analog to Kalman Filter
******* [[id:96ae9bce-587f-4e75-b7ec-e96c88212977][Optimal Control]]
******** Linear Systems
********* Linear-Quadratic-Regulator
- Linear-Quadratic Regulators are [[id:2296313e-9a11-4cfa-8899-d70b14cf8753][Feedback Controller]]s whose dynamics are *linear ODEs* but include a *quadratic cost*
  - *continuous-time analogs to Kalman Filters* with an additional cost function
  - special cases include linear-quadratic-Gaussian
********** Linear-Quadratic-Gaussian
- uses a Kalman Filter for *state estimation* and an LQR for *state feedback*
******** Nonlinear Systems
********* Model Predictive Control
******* Under what conditions can we show that the system is stable?
- If we can find a Control-Lyapunov Function
******** In what cases can we find a Control-Lyapunov function?
- People have used [[id:d0826af7-3c7e-4c4f-8019-921c8c7f6ff8][Neural Networks]] to learn one along with a verifier: https://arxiv.org/abs/2005.00611
******* Under what conditions can we show that the system is safe?
- If we use [[id:cd406cbd-e56a-4e57-8b40-cd69c3502f42][Control Barrier Function]]s
******** In what cases can we use a Control Barrier function?
- https://arxiv.org/abs/1903.11199
******* Continuous Action/Control
******** What can we do in continuous action spaces that we cannot do in discrete action spaces?
******* Discrete Action/Control
******** What can we do in discrete action spaces that we cannot do in continuous action spaces?
***** Discrete Space
****** Discrete-event Dynamic System
**** Discrete Time
***** Continuous Space
****** [[id:9f983716-9845-4d7b-9d9e-927182ad0a88][Classical Control]]
******* Linear Systems
******** Kalman Filter
- like [[id:296494de-31f1-49e9-b26f-67a8a856e618][linear gaussian model]]s but include a *linear control input* on the state process
******** Linear-Quadratic-Regulator
- can also be used in discrete time
******* Nonlinear Gaussian Systems
******** Extended Kalman Filter
******** Unscented Kalman Filter
****** Deterministic Markov Decision Process
****** Controlled Difference Equation
***** Discrete Space
****** Controlled Finite Automata (Video Games)
- dynamic programming
  - *policy iteration* and [[id:121304d1-9c38-4e69-b12f-d7987b62afc5][Value Iteration]] require a *perfect model of the MDP*, i.e. known dynamics and reward
    - https://www.baeldung.com/cs/ml-value-iteration-vs-policy-iteration
******* Under what conditions can we show that the system is safe?
- we can provably solve Pong (woohoo!) by using *symbolic state spaces* and synthesizing a decision-tree policy from a neural network policy
  - https://arxiv.org/pdf/1805.08328.pdf

* Questions
- What's the relationship between *stability* and *safety*?
  - https://arxiv.org/pdf/2309.05837.pdf
  - similarities:
    - if a *region of attraction* can be found (e.g. by using a [[id:29ae2cd5-d575-49d3-b1fe-2c9afd071a61][Lyapunov Function]]) and that region is disjoint from the [[id:e105f69c-375a-4fcb-a338-96340ebf21c5][Failure Domain]], then it is a [[id:4deb705f-da52-45ac-ab73-79161bdf371b][Safe Set]]
  - differences
    "Stability pertains to the systems ability (controller or not) to converge to an equilibrium point. Assymptotic stability means that the system as time goes to infinity the system is stable. In a controlled system, you can have a system that is unstable without the controller, but the controller is designed in such a way to make the closed loop system stable - fighter jets are such examples. Safety are best viewed as constraints, so just because a system is stable it does not necessarily mean it is safe. If there are trajectories heading towards the stable equilibrium point cross a safety constraint, then even though the system is stable it is not safe. So the question about safety is does there exist trajectories which violate the safety criteria. Now you can have a system which is safe but unstable, think of a spiraling outward oscillator. But a closed-loop system that is unstable for practical purposes are generally not of interest because the whole study of control is to get the system to a desired state or follow some desired trajectory. Designing a controller to have a closed loop system that is stable is a big part of control theory, which in the traditional way of going about it is with respect to a frequency analysis - in continuous time this is about designing a feedback controller whose real part of the poles (roots of a Laplace transform polynomial) are in the left hand plane. In discrete time the z-transform is used and then it's about ensuring that the poles are within the unit circle. These are directly related to being able to write down the dynamics as driven differential equations"
- How are Control-Lyapunov functions related to Control-Barrier functions?
  - CBFs were explicitly invented to combine (control) Lyapunov functions with [[id:a3928717-09cc-49ec-a24f-766c701916c0][Barrier Certificate]]s to attain both stability and safety beyond the boundary of the [[id:4deb705f-da52-45ac-ab73-79161bdf371b][Safe Set]].
    - In particular, Lyapunov functions have [[id:08c78d77-1ebf-43db-9b7c-cd896b5baf73][invariant level set]]s, so [[id:afdfeb9e-481f-4420-8ca9-890d05d407cc][Nagumo's Theorem]] gives us that there is a function where $\dot{h}\ge 0$ on the boundary of the invariant set. If we combine these level sets, then we get *invariance on the entire closed region*
    - see [[id:ccefedba-4bb5-43f0-be04-805e2882d51f][Ames et al. - 2019 - Control Barrier Functions Theory and Applications]] for a description of the history and relationship
      - Ames describes CBFs as the "safety dual" of CLFs (which are for stability)
- How are Lyapunov functions related to reach-avoid supermartingales (RASM)?
  - a RASM is a Lyapunov function "in expectation"
  - Lyapunov functions are non-increasing about the equilibrium point whereas RASMs are non-increasing *in expectation* about the target state
  - https://link.springer.com/content/pdf/10.1007/978-1-4757-3124-8_5.pdf
- What the heck is a Foster-Lyapunov function?
  - https://appliedprobability.blog/2018/06/22/foster-lyapunov/
- Is it even possible to verify neural networks?
  - https://arxiv.org/abs/2109.10317
- In what regimes are [[id:e288c7ff-72b4-4ae6-9d32-d298cc9f1f7e][PRISM (verification)]] and [[id:e288c7ff-72b4-4ae6-9d32-d298cc9f1f7e][Storm (Verification)]] applicable?
  - these model checkers can only be used in discrete, finite [[id:af01dbc9-f776-4dd7-8899-6f1e73fcb77c][MDP]]s, i.e. where the state and action spaces are discrete so that dynamic programming techniques such as [[id:121304d1-9c38-4e69-b12f-d7987b62afc5][Value Iteration]] can be run
- Even in the regimes where Prism/Storm are applicable, what is the *time complexity* with respect to the *size and dimensionality* of the state and action spaces?
  - the [[id:b18550c9-d1a9-40c0-a53c-3d5ab025004a][time complexity of value iteration]] is $O(|S|^2|A|)$ since, for each state, it iterates over *all possible actions and future states* $O(|S|\cdot|A|)$
    - https://ai.stackexchange.com/questions/9019/what-is-the-time-complexity-of-the-value-iteration-algorithm
    - https://arxiv.org/abs/1807.04920
- In what cases does using (linear, signal) temporal logic fail?
  - *any system without* discrete, finite, known dynamics
    - is this true?
- How have other people unified these ideas?
  - https://inria.hal.science/hal-02926215/document

* TODO
